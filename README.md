# become-data-engineer
Udacity 数据工程师纳米学位 (DEND)

## 完成的数据工程项目

### 1. 使用Postgres或Apache Cassandra进行数据建模

该实战项目的主题是学习和掌握数据建模，内容包含关系型数据库和非关系型数据库。对于关系型数据库，主要掌握范式设计和反范式的星型模型，雪花模型；对于非关系型数据库，主要了解CAP定理和列式存储。本人负责在项目 开发中将原始数据从csv或json文件读取出来，然后将数据清洗整理入库，形成OLAP数据模型。通过该实战项目学习和掌握数据建模的基本知识。 

### 2. 使用云数据仓库构建ETL管道

该实战项目的主题是学习和理解数据仓库的基础知识，以及掌握亚马逊云服务中S3（Simple Storage Service）和Redshift的使用。本人负责构建ETL数据管道，编写代码从S3中提取原始的云音乐数据，形成staging表存入Redshift，然后转换数据形成事实表和维度表组成的星型数据表，用作分析使用。为了完成该任务，我首先配置和启动Redshift集群，然后设计“事实表+维度表”的数据模型，编写数据仓库建表的相关语句，编写ETL数据管道代码。最后运行代码并分析数据仓库中的数据，并形成文档。通过该实战项目理解了什么是数据仓库，以及如何配合对象存储数据库搭建基本的OLAP系统。

### 3. 使用Spark构建ETL管道

该实战项目的主题是学习Spark的基本知识，如何使用Spark集群来实现大规模数据整理，以及如何使用AWS云服务搭建基本的数据湖。本人负责基于pyspark构建ETL数据管道，编写代码利用Spark从S3中提取云音乐业务数据，将其清洗整理，形成“事实表+维度表”的数据模型，然后将数据写回S3对象存储服务，形成数据湖。为了完成该任务，我首先使用Jupyter Notebook在小数据集上做了试验，调试成功后再迁移到大数据集上。最后编写文档，提供数据字典，并描述详细的使用方法。通过该实战项目理解什么是数据湖，以及如何使用Spark进行大数据处理。

### 4. 使用Apache Airflow构建ETL管道

项目描述的“STAR”原则：1）Situation（情景）简单项目背景，比如规模，软件功能和目标用户；2）Task（任务）自己完成的任务，要分清楚“参与”和“负责”；3）Action（行动）为了完成任务做了哪些工作，怎么做的；4）Result（结果）得到了什么成果，满足了什么需求，有什么贡献，摘自《剑指Offer》 

## 学位证书


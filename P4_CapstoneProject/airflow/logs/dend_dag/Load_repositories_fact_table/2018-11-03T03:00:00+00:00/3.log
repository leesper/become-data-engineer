[2019-11-17 16:41:15,523] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: dend_dag.Load_repositories_fact_table 2018-11-03T03:00:00+00:00 [queued]>
[2019-11-17 16:41:15,529] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: dend_dag.Load_repositories_fact_table 2018-11-03T03:00:00+00:00 [queued]>
[2019-11-17 16:41:15,529] {taskinstance.py:838} INFO - 
--------------------------------------------------------------------------------
[2019-11-17 16:41:15,529] {taskinstance.py:839} INFO - Starting attempt 3 of 4
[2019-11-17 16:41:15,529] {taskinstance.py:840} INFO - 
--------------------------------------------------------------------------------
[2019-11-17 16:41:15,533] {taskinstance.py:859} INFO - Executing <Task(LoadFactOperator): Load_repositories_fact_table> on 2018-11-03T03:00:00+00:00
[2019-11-17 16:41:15,533] {base_task_runner.py:133} INFO - Running: ['airflow', 'run', 'dend_dag', 'Load_repositories_fact_table', '2018-11-03T03:00:00+00:00', '--job_id', '259', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/dend_dag.py', '--cfg_path', '/var/folders/8q/nyxhvd6d55nfqjdbrybk6jpw0000gn/T/tmptz41sje4']
[2019-11-17 16:41:16,245] {base_task_runner.py:115} INFO - Job 259: Subtask Load_repositories_fact_table /Users/likejun/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
[2019-11-17 16:41:16,246] {base_task_runner.py:115} INFO - Job 259: Subtask Load_repositories_fact_table   """)
[2019-11-17 16:41:16,333] {base_task_runner.py:115} INFO - Job 259: Subtask Load_repositories_fact_table [2019-11-17 16:41:16,333] {__init__.py:51} INFO - Using executor SequentialExecutor
[2019-11-17 16:41:16,634] {base_task_runner.py:115} INFO - Job 259: Subtask Load_repositories_fact_table [2019-11-17 16:41:16,634] {dagbag.py:90} INFO - Filling up the DagBag from /Users/likejun/airflow/dags/dend_dag.py
[2019-11-17 16:41:16,649] {base_task_runner.py:115} INFO - Job 259: Subtask Load_repositories_fact_table [2019-11-17 16:41:16,648] {cli.py:516} INFO - Running <TaskInstance: dend_dag.Load_repositories_fact_table 2018-11-03T03:00:00+00:00 [running]> on host likejundeMacBook-Pro.local
[2019-11-17 16:41:16,655] {load_facts.py:21} INFO - LoadFactOperator for repository_fact
[2019-11-17 16:41:16,663] {logging_mixin.py:95} INFO - [[34m2019-11-17 16:41:16,663[0m] {[34mbase_hook.py:[0m84} INFO[0m - Using connection to: [1mid: redshift. Host: redshift-cluster.chjcdyihped1.us-west-2.redshift.amazonaws.com, Port: 5439, Schema: dev, Login: awsuser, Password: XXXXXXXX, extra: {}[0m[0m
[2019-11-17 16:41:20,819] {logging_mixin.py:95} INFO - [[34m2019-11-17 16:41:20,819[0m] {[34mdbapi_hook.py:[0m171} INFO[0m - 
    INSERT INTO repository_fact
        (SELECT
            sp."repository id" AS repository_id,
            sp."repository stars count" AS stars,
            sp."repository forks count" AS forks,
            sp."repository watchers count" AS watchers,
            sp."repository contributors count" AS contributors,
            sp."repository size" AS size,
            sp."repository name with owner" AS repo,
            sv.id AS version_id,
            sv.project_id,
            sp."repository created timestamp" AS create_time,
            sd.id AS dependency_id
            FROM staging_projects sp
            JOIN staging_versions sv
            ON sp.id = sv.project_id
            JOIN staging_dependencies sd
            ON sp.id = sd."project id"
            WHERE sp."repository id" IS NOT NULL 
            AND sp."repository created timestamp" IS NOT NULL)
    [0m
[2019-11-17 16:52:46,969] {taskinstance.py:1051} ERROR - SSL SYSCALL error: EOF detected
Traceback (most recent call last):
  File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/likejun/airflow/plugins/operators/load_facts.py", line 30, in execute
    pg_hook.run(SqlQueries.repository_table_select)
  File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 172, in run
    cur.execute(s)
psycopg2.OperationalError: SSL SYSCALL error: EOF detected

[2019-11-17 16:52:46,975] {taskinstance.py:1074} INFO - Marking task as UP_FOR_RETRY
[2019-11-17 17:32:20,120] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: dend_dag.Load_repositories_fact_table 2018-11-03T03:00:00+00:00 [queued]>
[2019-11-17 17:32:20,126] {taskinstance.py:620} INFO - Dependencies all met for <TaskInstance: dend_dag.Load_repositories_fact_table 2018-11-03T03:00:00+00:00 [queued]>
[2019-11-17 17:32:20,127] {taskinstance.py:838} INFO - 
--------------------------------------------------------------------------------
[2019-11-17 17:32:20,127] {taskinstance.py:839} INFO - Starting attempt 3 of 4
[2019-11-17 17:32:20,127] {taskinstance.py:840} INFO - 
--------------------------------------------------------------------------------
[2019-11-17 17:32:20,131] {taskinstance.py:859} INFO - Executing <Task(LoadFactOperator): Load_repositories_fact_table> on 2018-11-03T03:00:00+00:00
[2019-11-17 17:32:20,131] {base_task_runner.py:133} INFO - Running: ['airflow', 'run', 'dend_dag', 'Load_repositories_fact_table', '2018-11-03T03:00:00+00:00', '--job_id', '261', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/dend_dag.py', '--cfg_path', '/var/folders/8q/nyxhvd6d55nfqjdbrybk6jpw0000gn/T/tmpt2lmb9to']
[2019-11-17 17:32:20,777] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table /Users/likejun/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use "pip install psycopg2-binary" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.
[2019-11-17 17:32:20,777] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   """)
[2019-11-17 17:32:20,859] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table [2019-11-17 17:32:20,858] {__init__.py:51} INFO - Using executor SequentialExecutor
[2019-11-17 17:32:21,133] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table [2019-11-17 17:32:21,132] {dagbag.py:90} INFO - Filling up the DagBag from /Users/likejun/airflow/dags/dend_dag.py
[2019-11-17 17:32:21,146] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table [2019-11-17 17:32:21,146] {cli.py:516} INFO - Running <TaskInstance: dend_dag.Load_repositories_fact_table 2018-11-03T03:00:00+00:00 [running]> on host likejundeMacBook-Pro.local
[2019-11-17 17:32:21,152] {load_facts.py:21} INFO - LoadFactOperator for repository_fact
[2019-11-17 17:32:21,163] {logging_mixin.py:95} INFO - [[34m2019-11-17 17:32:21,162[0m] {[34mbase_hook.py:[0m84} INFO[0m - Using connection to: [1mid: redshift. Host: redshift-cluster.chjcdyihped1.us-west-2.redshift.amazonaws.com, Port: 5439, Schema: dev, Login: awsuser, Password: XXXXXXXX, extra: {}[0m[0m
[2019-11-17 17:33:36,412] {taskinstance.py:1051} ERROR - could not connect to server: Operation timed out
	Is the server running on host "redshift-cluster.chjcdyihped1.us-west-2.redshift.amazonaws.com" (44.227.86.140) and accepting
	TCP/IP connections on port 5439?
Traceback (most recent call last):
  File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/Users/likejun/airflow/plugins/operators/load_facts.py", line 31, in execute
    pg_hook.run(stmt)
  File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 159, in run
    with closing(self.get_conn()) as conn:
  File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/hooks/postgres_hook.py", line 75, in get_conn
    self.conn = psycopg2.connect(**conn_args)
  File "/Users/likejun/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Operation timed out
	Is the server running on host "redshift-cluster.chjcdyihped1.us-west-2.redshift.amazonaws.com" (44.227.86.140) and accepting
	TCP/IP connections on port 5439?

[2019-11-17 17:33:36,421] {taskinstance.py:1074} INFO - Marking task as UP_FOR_RETRY
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table Traceback (most recent call last):
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/bin/airflow", line 32, in <module>
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     args.func(args)
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     return f(*args, **kwargs)
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/bin/cli.py", line 522, in run
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     _run(args, dag, ti)
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/bin/cli.py", line 440, in _run
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     pool=args.pool,
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/utils/db.py", line 74, in wrapper
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     return func(*args, **kwargs)
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 926, in _run_raw_task
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     result = task_copy.execute(context=context)
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/airflow/plugins/operators/load_facts.py", line 31, in execute
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     pg_hook.run(stmt)
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/hooks/dbapi_hook.py", line 159, in run
[2019-11-17 17:33:36,437] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     with closing(self.get_conn()) as conn:
[2019-11-17 17:33:36,438] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/lib/python3.7/site-packages/airflow/hooks/postgres_hook.py", line 75, in get_conn
[2019-11-17 17:33:36,438] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     self.conn = psycopg2.connect(**conn_args)
[2019-11-17 17:33:36,438] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table   File "/Users/likejun/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py", line 130, in connect
[2019-11-17 17:33:36,438] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
[2019-11-17 17:33:36,438] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table psycopg2.OperationalError: could not connect to server: Operation timed out
[2019-11-17 17:33:36,438] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table 	Is the server running on host "redshift-cluster.chjcdyihped1.us-west-2.redshift.amazonaws.com" (44.227.86.140) and accepting
[2019-11-17 17:33:36,438] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table 	TCP/IP connections on port 5439?
[2019-11-17 17:33:36,438] {base_task_runner.py:115} INFO - Job 261: Subtask Load_repositories_fact_table 
[2019-11-17 17:33:40,226] {logging_mixin.py:95} INFO - [[34m2019-11-17 17:33:40,226[0m] {[34mlocal_task_job.py:[0m105} INFO[0m - Task exited with return code 1[0m

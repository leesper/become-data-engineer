1. 研究项目提供的开放数据集，确定项目的范围，完成相关文档撰写
2. 下载数据，考虑到题目的要求必须是两个数据源且要有不同的数据格式，所以将Github API考虑在内
3. 把下载好的数据上传到AWS S3
4. 一开始是考虑抽取数据样本先进行EDA，后来仔细研究数据之后觉得还不如直接用Spark分析全量数据，所以决定启动EMR集群，通过notebook来对数据进行EDA

	对数据进行质量检查，文档记录数据清洗加工的处理过程
	确认要从Github API补充什么数据
5. 通过对数据的研究确认要从GithubAPI获取和采集哪些数据（JSON格式）
	
	编写程序从GithubAPI采集JSON格式数据
	将JSON文件上传到S3

6. 对数据进行模型设计，确定事实表和维度表，解释所选择数据模型的合理性
    
	复习数据仓库相关知识点
7. 根据数据模型进行数据管道概念设计
8. 数据管道编排  

	弄清S3/Redshift/EMR之间的权限访问控制（IAM）
    	弄清Airflow如何与EMR中的Spark交互
    	从S3读入数据文件形成staging表
    	通过staging筛选出Github项目，然后构建facts & dimensions
    	在数据管道中进行数据质量检查
        		完整性检查
        		单元测试代码
        		source/count检查

9. 提供最终数据的数据字典

notes:
1. 使用S3存储原始文件，使用Redshift做数据仓库，用EMR的Spark集群做计算，用Airflow做管道编排
2. Redshift集群要attach一个role来访问S3，同样EMR也要attach一个role访问Redshift，Airflow要通过IAM user访问EMR和Redshift

10. 项目应用的目标（根据上面对数据的研究确定最终产品形态）
    	为什么选择这个数据模型

11. 进行项目架构设计
   	架构设计
    	技术选型合理性
    	性能指标
        		SLA
        		数据量增大100x怎么办
        		100+DB访问量如何处理
    	解释数据的更新频率
    	每日七点定时运行如何实现
    	后端开发
        		web框架技术选型
        		运行哪些查询？
    	前端开发
    	云端部署

12. 整理成博客并附上链接

